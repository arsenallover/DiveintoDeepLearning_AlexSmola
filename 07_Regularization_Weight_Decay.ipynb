{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600071225089",
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = 5 + 1.2*x -3.4*x^2 + 5.6*x^3 + ... $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "labels = y; \n",
    "poly_features = x and its poly featues\n",
    "true_w = coefficients\n",
    "features = random variable '''\n",
    "\n",
    "max_degrees = 20\n",
    "n_train, n_test = 100, 100 \n",
    "true_w = np.zeros(max_degrees)  # Allocate lots of space\n",
    "true_w[:4] = [5, 1.2, -3.4, 5.6]  # \n",
    "\n",
    "features = np.random.normal(size = (n_train + n_test, 1))\n",
    "np.random.shuffle(features)\n",
    "poly_features = np.power(features, np.arange(max_degrees).reshape(1, -1))\n",
    "labels = np.dot(poly_features, true_w)\n",
    "labels += np.random.normal(scale=0.1, size=labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from NumPy ndarrays to tensors\n",
    "true_w, features, poly_features, labels = [tf.constant(x, dtype=\n",
    "    tf.float32) for x in [true_w, features, poly_features, labels]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will overfit the model and then check power of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, batch_size, is_train = False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(buffer_size= 100)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch : 0, training_loss : 42848067584.0\nepoch : 1, training_loss : 134169427968.0\nepoch : 2, training_loss : 266419142656.0\nepoch : 3, training_loss : 18056912896.0\nepoch : 4, training_loss : 115343327232.0\nepoch : 5, training_loss : 38512803840.0\nepoch : 6, training_loss : 8449315328.0\nepoch : 7, training_loss : 7957939712.0\nepoch : 8, training_loss : 9280643072.0\nepoch : 9, training_loss : 5912075776.0\nepoch : 10, training_loss : 4672243712.0\nepoch : 11, training_loss : 3987857664.0\nepoch : 12, training_loss : 3440613120.0\nepoch : 13, training_loss : 2930254080.0\nepoch : 14, training_loss : 2612780800.0\nepoch : 15, training_loss : 2198679296.0\nepoch : 16, training_loss : 2302754560.0\nepoch : 17, training_loss : 2211642112.0\nepoch : 18, training_loss : 1501234304.0\nepoch : 19, training_loss : 1186942720.0\nepoch : 20, training_loss : 657969408.0\nepoch : 21, training_loss : 16661351424.0\nepoch : 22, training_loss : 46075080704.0\nepoch : 23, training_loss : 7378688000.0\nepoch : 24, training_loss : 3471083520.0\nepoch : 25, training_loss : 10451605504.0\nepoch : 26, training_loss : 435038976.0\nepoch : 27, training_loss : 87193706496.0\nepoch : 28, training_loss : 271039741952.0\nepoch : 29, training_loss : 985329762304.0\nepoch : 30, training_loss : 170395451392.0\nepoch : 31, training_loss : 849024712704.0\nepoch : 32, training_loss : 962073264128.0\nepoch : 33, training_loss : 609784496128.0\nepoch : 34, training_loss : 969656631296.0\nepoch : 35, training_loss : 47411662848.0\nepoch : 36, training_loss : 120760188928.0\nepoch : 37, training_loss : 44614152192.0\nepoch : 38, training_loss : 80109720.0\nepoch : 39, training_loss : 1354421760.0\nepoch : 40, training_loss : 325095744.0\nepoch : 41, training_loss : 74798904.0\nepoch : 42, training_loss : 221177408.0\nepoch : 43, training_loss : 77129472.0\nepoch : 44, training_loss : 425024832.0\nepoch : 45, training_loss : 86105744.0\nepoch : 46, training_loss : 132746960.0\nepoch : 47, training_loss : 1121842304.0\nepoch : 48, training_loss : 86415688.0\nepoch : 49, training_loss : 2622767872.0\n"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.RandomNormal(stddev  = 0.1))\n",
    "])\n",
    "\n",
    "num_epochs, batch_size, lr = 50, 10, 0.05\n",
    "\n",
    "data_iter = load_data((poly_features[:n_train, :], labels[:n_train]), batch_size, is_train = True)\n",
    "training_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = net(X)\n",
    "            mse_loss = loss(yhat, y)\n",
    "        \n",
    "        grads = tape.gradient(mse_loss, net.trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, net.trainable_variables))\n",
    "\n",
    "    epoch_loss = loss(net(poly_features[:n_train, :]), labels[:n_train])\n",
    "    training_loss.append(epoch_loss)\n",
    "    print(f\"epoch : {epoch}, training_loss : {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_zero_lambda = tf.norm(net.get_weights()[0]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularize the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch : 0, training_loss : 2107365130240.0\nepoch : 1, training_loss : 9230185472.0\nepoch : 2, training_loss : 1306311131136.0\nepoch : 3, training_loss : 2281608704.0\nepoch : 4, training_loss : 131580116992.0\nepoch : 5, training_loss : 77246734336.0\nepoch : 6, training_loss : 79104098304.0\nepoch : 7, training_loss : 247983712.0\nepoch : 8, training_loss : 981275840.0\nepoch : 9, training_loss : 334350656.0\nepoch : 10, training_loss : 122329152.0\nepoch : 11, training_loss : 125269320.0\nepoch : 12, training_loss : 201294112.0\nepoch : 13, training_loss : 137106544.0\nepoch : 14, training_loss : 76828536.0\nepoch : 15, training_loss : 627702528.0\nepoch : 16, training_loss : 389540096.0\nepoch : 17, training_loss : 210110688.0\nepoch : 18, training_loss : 2044560512.0\nepoch : 19, training_loss : 4505469440.0\nepoch : 20, training_loss : 7682944512.0\nepoch : 21, training_loss : 16359603200.0\nepoch : 22, training_loss : 34123872256.0\nepoch : 23, training_loss : 39241134080.0\nepoch : 24, training_loss : 71524630528.0\nepoch : 25, training_loss : 7321513984.0\nepoch : 26, training_loss : 321019456.0\nepoch : 27, training_loss : 3394590464.0\nepoch : 28, training_loss : 13188759552.0\nepoch : 29, training_loss : 32865187840.0\nepoch : 30, training_loss : 28446091264.0\nepoch : 31, training_loss : 206361608192.0\nepoch : 32, training_loss : 217013059584.0\nepoch : 33, training_loss : 50148970496.0\nepoch : 34, training_loss : 50090573824.0\nepoch : 35, training_loss : 96687988736.0\nepoch : 36, training_loss : 13229081600.0\nepoch : 37, training_loss : 231987806208.0\nepoch : 38, training_loss : 404799619072.0\nepoch : 39, training_loss : 417884897280.0\nepoch : 40, training_loss : 326188662784.0\nepoch : 41, training_loss : 14690048000.0\nepoch : 42, training_loss : 838946979840.0\nepoch : 43, training_loss : 394608771072.0\nepoch : 44, training_loss : 196492820480.0\nepoch : 45, training_loss : 25194536960.0\nepoch : 46, training_loss : 732362244096.0\nepoch : 47, training_loss : 34806349824.0\nepoch : 48, training_loss : 403923730432.0\nepoch : 49, training_loss : 140687015936.0\n"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.RandomNormal(stddev  = 0.1))\n",
    "])\n",
    "\n",
    "def l2_penalty(w):\n",
    "    return tf.reduce_sum(tf.pow(w, 2)) / 2\n",
    "\n",
    "num_epochs, batch_size, lr, lambd = 50, 10, 0.05, 3\n",
    "\n",
    "data_iter = load_data((poly_features[:n_train, :], labels[:n_train]), batch_size, is_train = True)\n",
    "training_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = net(X)\n",
    "            mse_loss = loss(yhat, y) + lambd*l2_penalty(net.trainable_variables[0])\n",
    "        \n",
    "        grads = tape.gradient(mse_loss, net.trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, net.trainable_variables))\n",
    "\n",
    "    epoch_loss = loss(net(poly_features[:n_train, :]), labels[:n_train])\n",
    "    training_loss.append(epoch_loss)\n",
    "    print(f\"epoch : {epoch}, training_loss : {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_with_lambda = tf.norm(net.get_weights()[0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.44223258 1.2648956\n"
    }
   ],
   "source": [
    "print(norm_with_lambda, norm_zero_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\n",
    "true_w, true_b = tf.ones((num_inputs, 1)) * 0.01, 0.05\n",
    "train_data = d2l.synthetic_data(true_w, true_b, n_train)\n",
    "train_iter = d2l.load_array(train_data, batch_size)\n",
    "test_data = d2l.synthetic_data(true_w, true_b, n_test)\n",
    "test_iter = d2l.load_array(test_data, batch_size, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    w = tf.Variable(tf.random.normal(mean=1, shape=(num_inputs, 1)))\n",
    "    b = tf.Variable(tf.zeros(shape=(1, )))\n",
    "    return [w, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(w):\n",
    "    return tf.reduce_sum(tf.pow(w, 2)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lambd):\n",
    "    w, b = init_params()\n",
    "    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n",
    "    num_epochs, lr = 100, 0.003\n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            with tf.GradientTape() as tape:\n",
    "                l = loss(net(X), y) + lambd * l2_penalty(w)\n",
    "            grads = tape.gradient(l, [w, b])\n",
    "            d2l.sgd([w, b], grads, lr, batch_size)\n",
    "    print('L2 norm of w:', tf.norm(w).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "L2 norm of w: 0.572348\n"
    }
   ],
   "source": [
    "train(lambd=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "L2 norm of w: 19.757078\n"
    }
   ],
   "source": [
    "train(lambd=0)"
   ]
  }
 ]
}