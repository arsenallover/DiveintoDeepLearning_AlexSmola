{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600079777489",
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X, p):\n",
    "    ''' This function performs dropouts on X '''\n",
    "    assert 0 <= p <= 1\n",
    "    if p == 0:\n",
    "        # Drop all & make it zero\n",
    "        return tf.zeros_like(X).numpy()\n",
    "    if p == 1:\n",
    "        # Return X as it\n",
    "        return X.numpy()\n",
    "    \n",
    "    # Create bool of % of instances to drop\n",
    "    mask = tf.random.uniform(shape = tf.shape(X), minval = 0, maxval = 1) < 1 - p\n",
    "    print(f\" Mask : {mask.numpy()}\")\n",
    "    # Convert Bool to binary & Retain reduced X\n",
    "    print(f\" Masked X : {(tf.cast(mask, dtype = tf.float32)*X).numpy()}\")\n",
    "    # Multiply with X and normalize by fraction retained\n",
    "    return (tf.cast(mask, dtype = tf.float32)* X / (1 - p)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.random.uniform(shape = (1, 10), minval=0, maxval=10, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X : [[3.3110595 3.9678252 4.6132755 3.1029058 1.7998374 1.3511956 2.1178722\n  1.3179338 7.7726364 3.8459766]]\n Mask : [[ True False  True  True False False False  True False  True]]\n Masked X : [[3.3110595 0.        4.6132755 3.1029058 0.        0.        0.\n  1.3179338 0.        3.8459766]]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[6.622119 , 0.       , 9.226551 , 6.2058115, 0.       , 0.       ,\n        0.       , 2.6358676, 0.       , 7.691953 ]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "print(f\"X : {X.numpy()}\")\n",
    "dropout_layer(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X, p):\n",
    "    ''' This function performs dropouts on X '''\n",
    "    assert 0 <= p <= 1\n",
    "    if p == 0:\n",
    "        return tf.zeros_like(X)\n",
    "    if p == 1:\n",
    "        return X\n",
    "    mask = tf.random.uniform(shape = tf.shape(X), minval = 0, maxval = 1) < 1 - p\n",
    "    return (tf.cast(mask, dtype = tf.float32)* X / (1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(\n[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n [ 8.  9. 10. 11. 12. 13. 14. 15.]], shape=(2, 8), dtype=float32)\ntf.Tensor(\n[[0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32)\ntf.Tensor(\n[[ 0.  0.  0.  6.  0.  0.  0.  0.]\n [16.  0. 20. 22. 24. 26.  0. 30.]], shape=(2, 8), dtype=float32)\ntf.Tensor(\n[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n [ 8.  9. 10. 11. 12. 13. 14. 15.]], shape=(2, 8), dtype=float32)\n"
    }
   ],
   "source": [
    "X = tf.reshape(tf.range(16, dtype=tf.float32), (2, 8))\n",
    "print(X)\n",
    "print(dropout_layer(X, 0.))\n",
    "print(dropout_layer(X, 0.5))\n",
    "print(dropout_layer(X, 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout in NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout1, dropout2 = 0.5, 0.5\n",
    "\n",
    "class Net(tf.keras.Model):\n",
    "    def __init__(self, num_outputs, num_hidden1, num_hidden2):\n",
    "        super().__init__()\n",
    "        self.input_layer = tf.keras.layers.Flatten()\n",
    "        self.hidden1 = tf.keras.layers.Dense(num_hidden1, activation = 'relu')\n",
    "        self.hidden2 = tf.keras.layers.Dense(num_hidden2, activation = 'relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(num_outputs)\n",
    "\n",
    "    def call(self, inputs, training = None):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.hidden1(x)\n",
    "        if training:\n",
    "            x = dropout_layer(x, dropout1)\n",
    "        x = self.hidden2(x)\n",
    "        if training:\n",
    "            x = dropout_layer(x, dropout2)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "num_outputs, num_hidden1, num_hidden2 = 10, 256, 256    \n",
    "net = Net(num_outputs, num_hidden1, num_hidden2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = tf.cast(tf.reshape(train_features, (-1, 784)), tf.float32)/255.0\n",
    "test_features = tf.cast(tf.reshape(test_features, (-1, 784)), tf.float32)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, batch_size, is_train = True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(buffer_size = 1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr, batch_size = 50, 0.5, 256\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch : 0, loss : 0.5674312710762024\nepoch : 1, loss : 0.4548048973083496\nepoch : 2, loss : 0.42549094557762146\nepoch : 3, loss : 0.41688743233680725\nepoch : 4, loss : 0.4007378816604614\nepoch : 5, loss : 0.3832167387008667\nepoch : 6, loss : 0.4032727777957916\nepoch : 7, loss : 0.35775741934776306\nepoch : 8, loss : 0.332672655582428\nepoch : 9, loss : 0.3756335973739624\nepoch : 10, loss : 0.330614298582077\nepoch : 11, loss : 0.35788366198539734\nepoch : 12, loss : 0.33307766914367676\nepoch : 13, loss : 0.33154016733169556\nepoch : 14, loss : 0.29667723178863525\nepoch : 15, loss : 0.34693488478660583\nepoch : 16, loss : 0.31189972162246704\nepoch : 17, loss : 0.33852487802505493\nepoch : 18, loss : 0.311088502407074\nepoch : 19, loss : 0.3039800822734833\nepoch : 20, loss : 0.2947123050689697\nepoch : 21, loss : 0.2882457971572876\nepoch : 22, loss : 0.3064858019351959\nepoch : 23, loss : 0.32587528228759766\nepoch : 24, loss : 0.28363892436027527\nepoch : 25, loss : 0.2855738699436188\nepoch : 26, loss : 0.28089961409568787\nepoch : 27, loss : 0.2773718237876892\nepoch : 28, loss : 0.2760830521583557\nepoch : 29, loss : 0.27967193722724915\nepoch : 30, loss : 0.26636427640914917\nepoch : 31, loss : 0.27056899666786194\nepoch : 32, loss : 0.2539340853691101\nepoch : 33, loss : 0.25527456402778625\nepoch : 34, loss : 0.26047205924987793\nepoch : 35, loss : 0.26282837986946106\nepoch : 36, loss : 0.28646016120910645\nepoch : 37, loss : 0.2548937201499939\nepoch : 38, loss : 0.25720492005348206\nepoch : 39, loss : 0.26384830474853516\nepoch : 40, loss : 0.2457163780927658\nepoch : 41, loss : 0.2928914725780487\nepoch : 42, loss : 0.24971669912338257\nepoch : 43, loss : 0.2500614523887634\nepoch : 44, loss : 0.23600097000598907\nepoch : 45, loss : 0.25504666566848755\nepoch : 46, loss : 0.28775927424430847\nepoch : 47, loss : 0.2501925230026245\nepoch : 48, loss : 0.24229316413402557\nepoch : 49, loss : 0.25787612795829773\n"
    }
   ],
   "source": [
    "data_iter = load_data((train_features, train_labels), batch_size)\n",
    "\n",
    "training_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "            \n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = net(X, training = True)\n",
    "            ce_loss = loss(y, yhat)\n",
    "\n",
    "        grads = tape.gradient(ce_loss, net.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, net.trainable_variables))\n",
    "\n",
    "    epoch_loss = loss(train_labels, net(train_features))\n",
    "    training_loss.append(epoch_loss)\n",
    "    print(f\"epoch : {epoch}, loss : {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8839"
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ytest_pred = np.argmax(net(test_features), axis = 1)\n",
    "accuracy_score(ytest_pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8485"
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ytest_pred = np.argmax(net(test_features), axis = 1)\n",
    "accuracy_score(ytest_pred, test_labels)"
   ]
  }
 ]
}