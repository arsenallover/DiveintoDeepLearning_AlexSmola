{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "display_name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "metadata": {
    "interpreter": {
     "hash": "22b3a9e511cc71c7e8c0c199ed3ba2f3f697f01009167188cfc9ddfa1a3c5b27"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters = 6, kernel_size=5, activation='sigmoid', padding='same'),\n",
    "        tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Conv2D(filters = 16, kernel_size=5, activation='sigmoid'),\n",
    "        tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(120, activation = 'sigmoid'),\n",
    "        tf.keras.layers.Dense(84, activation = 'sigmoid'),\n",
    "        tf.keras.layers.Dense(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Conv2D output_shape: \t (1, 28, 28, 6)\nAveragePooling2D output_shape: \t (1, 14, 14, 6)\nConv2D output_shape: \t (1, 10, 10, 16)\nAveragePooling2D output_shape: \t (1, 5, 5, 16)\nFlatten output_shape: \t (1, 400)\nDense output_shape: \t (1, 120)\nDense output_shape: \t (1, 84)\nDense output_shape: \t (1, 10)\n"
    }
   ],
   "source": [
    "X = tf.random.uniform((1, 28, 28, 1))\n",
    "for layer in net().layers:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output_shape: \\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, net, train_iter, test_iter, num_epochs):\n",
    "        self.net = net\n",
    "        self.train_iter = train_iter\n",
    "        self.test_iter = test_iter\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        test_acc = self.net.evaluate(self.test_iter, verbose = 0)\n",
    "        metrics = (logs[\"loss\"], logs[\"accuracy\"], test_acc[1])\n",
    "        print(f'epoch {epoch}, loss {metrics[0]:.3f}, train acc {metrics[1]:.3f}, 'f'test acc {metrics[2]:.3f}')\n",
    "\n",
    "        if epoch == self.num_epochs - 1:\n",
    "            print(f'loss {metrics[0]:.3f}, train acc {metrics[1]:.3f}, 'f'test acc {metrics[2]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None):   #@save\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    # Divide all numbers by 255 so that all pixel values are between\n",
    "    # 0 and 1, add a batch dimension at the last. And cast label to int32\n",
    "    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,\n",
    "                            tf.cast(y, dtype='int32'))\n",
    "    resize_fn = lambda X, y: (\n",
    "        tf.image.resize_with_pad(X, resize, resize) if resize else X, y)\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(\n",
    "            batch_size).shuffle(len(mnist_train[0])).map(resize_fn),\n",
    "        tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(\n",
    "            batch_size).map(resize_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch 0, loss 2.320, train acc 0.102, test acc 0.100\nepoch 1, loss 1.632, train acc 0.366, test acc 0.585\nepoch 2, loss 0.886, train acc 0.649, test acc 0.662\nepoch 3, loss 0.717, train acc 0.720, test acc 0.718\nepoch 4, loss 0.647, train acc 0.747, test acc 0.763\nepoch 5, loss 0.592, train acc 0.771, test acc 0.777\nepoch 6, loss 0.545, train acc 0.791, test acc 0.789\nepoch 7, loss 0.512, train acc 0.803, test acc 0.795\nepoch 8, loss 0.488, train acc 0.815, test acc 0.799\nepoch 9, loss 0.470, train acc 0.823, test acc 0.803\nloss 0.470, train acc 0.823, test acc 0.803\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tensorflow.python.keras.callbacks.History at 0x2fb9b8c8&gt;"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.9, 10, 256\n",
    "\n",
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "with strategy.scope():\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    net_model = net()\n",
    "    net_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "callback = TrainCallback(net_model, train_iter, test_iter, num_epochs)\n",
    "net_model.fit(train_iter, epochs=num_epochs, verbose=0, callbacks=[callback])"
   ]
  }
 ]
}