{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "* Softmax Function\n",
    "    * Ensures the values are non-negative (exponentiating the values)\n",
    "    * Smooth prob output btw 0-1\n",
    "    * Differentiable\n",
    "    * This model mapped our inputs directly to our outputs via a single affine transformation, followed by a softmax operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "train_features = tf.reshape(train_features, (-1, num_inputs))\n",
    "train_features = tf.cast(train_features, tf.float32)/255.0\n",
    "test_features = tf.reshape(test_features, (-1, num_inputs))\n",
    "test_features = tf.cast(test_features, tf.float32)/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(buffer_size = 1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "# next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(num_inputs, num_outputs):   \n",
    "    w = tf.Variable(tf.random.normal(shape = (num_inputs, num_outputs),\n",
    "    mean = 0, stddev = 0.01), trainable = True)\n",
    "    b = tf.Variable(tf.zeros(num_outputs), trainable = True)\n",
    "    return w, b\n",
    "    \n",
    "## Softmax Function\n",
    "def softmax(x):\n",
    "    ''' x is list of sigmoid outputs '''\n",
    "    x_exp = tf.exp(x)\n",
    "    exp_norm = tf.reduce_sum(x_exp, 1, keepdims= True)\n",
    "    return x_exp/exp_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_multiclass(x, w, b):\n",
    "    ''' \n",
    "    Multiclass classification implies softmax instead of sigmoid \n",
    "    Inputs:\n",
    "    X : shape(num_examples, 28, 28)\n",
    "    w : shape(784, 10)\n",
    "    b : shape(10, 1)\n",
    "    '''\n",
    "    x = tf.reshape(x, (-1, w.shape[0]))\n",
    "    output = tf.matmul(x, w) + b\n",
    "    softmax_output = softmax(output)\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy_loss(yhat, y):\n",
    "    ''' This function calculates the prediction prob of the correct class & takes log '''\n",
    "    # depth corresponds to number of classes\n",
    "    y_one_hot = tf.one_hot(y, depth = yhat.shape[-1])  \n",
    "    \n",
    "    # Masking only correct prediction prob\n",
    "    prob = tf.boolean_mask(yhat, y_one_hot)\n",
    "\n",
    "    # ce : log(y*yhat) but y is always 1\n",
    "    return -tf.math.log(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, grads, lr, batch_size):\n",
    "    for param, grad in zip(params, grads):\n",
    "        param.assign_sub(lr*grad/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "lr = 0.05\n",
    "batch_size = 256\n",
    "\n",
    "w, b = init_params(num_inputs, num_outputs)\n",
    "data_iter = load_data((train_features, train_labels), batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = log_reg_multiclass(X, w, b)\n",
    "            loss = crossentropy_loss(yhat, y)\n",
    "        # print(loss)\n",
    "        \n",
    "        # Calculate Gradients\n",
    "        dw, db = tape.gradient(loss, [w, b])\n",
    "        \n",
    "        # Update Gradients\n",
    "        sgd([w, b], [dw, db], lr, batch_size)\n",
    "    \n",
    "    training_loss = crossentropy_loss(log_reg_multiclass(train_features, w, b), train_labels)\n",
    "    print(f\"epoch : {epoch}, training_loss : {tf.reduce_sum(training_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_pred = log_reg_multiclass(test_features, w, b)\n",
    "test_pred = np.argmax(test_label_pred, axis = 1)\n",
    "accuracy_score(test_pred, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "# train_features = tf.reshape(train_features, (-1, num_inputs))\n",
    "# train_features = tf.cast(train_features, tf.float32)/255.0\n",
    "# test_features = tf.reshape(test_features, (-1, num_inputs))\n",
    "# test_features = tf.cast(test_features, tf.float32)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.initializers.RandomNormal(stddev = 0.1)\n",
    "log_reg_tf = tf.keras.Sequential()\n",
    "log_reg_tf.add(tf.keras.layers.Flatten(input_shape = (28, 28)))\n",
    "log_reg_tf.add(tf.keras.layers.Dense(10, kernel_initializer=initializer, activation  = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = load_data((train_features, train_labels), batch_size)\n",
    "\n",
    "num_epochs = 50\n",
    "lr = 0.05\n",
    "batch_size = 256\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = np.argmax(log_reg_tf(X, training = True), axis = 1)\n",
    "            yhat_ohe = tf.one_hot(yhat, num_outputs)\n",
    "            ce_loss = loss(y, yhat_ohe)\n",
    "        \n",
    "        grads = tape.gradient(ce_loss, log_reg_tf.trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, log_reg_tf.trainable_variables))\n",
    "\n",
    "    yhat = np.argmax(log_reg_tf(train_features), axis = 1)\n",
    "    training_loss = loss(yhat, y)\n",
    "    print(f\"epoch : {epoch}, training_loss : {tf.reduce_sum(training_loss)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1600003121561"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}