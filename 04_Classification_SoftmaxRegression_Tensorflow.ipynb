{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "* Softmax Function\n",
    "    * Ensures the values are non-negative (exponentiating the values)\n",
    "    * Smooth prob output btw 0-1\n",
    "    * Differentiable\n",
    "    * This model mapped our inputs directly to our outputs via a single affine transformation, followed by a softmax operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "train_features = tf.reshape(train_features, (-1, num_inputs))\n",
    "train_features = tf.cast(train_features, tf.float32)/255.0\n",
    "test_features = tf.reshape(test_features, (-1, num_inputs))\n",
    "test_features = tf.cast(test_features, tf.float32)/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(buffer_size = 1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "# next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(num_inputs, num_outputs):   \n",
    "    w = tf.Variable(tf.random.normal(shape = (num_inputs, num_outputs),\n",
    "    mean = 0, stddev = 0.01), trainable = True)\n",
    "    b = tf.Variable(tf.zeros(num_outputs), trainable = True)\n",
    "    return w, b\n",
    "    \n",
    "## Softmax Function\n",
    "def softmax(x):\n",
    "    ''' x is list of sigmoid outputs '''\n",
    "    x_exp = tf.exp(x)\n",
    "    exp_norm = tf.reduce_sum(x_exp, 1, keepdims= True)\n",
    "    return x_exp/exp_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_multiclass(x, w, b):\n",
    "    ''' \n",
    "    Multiclass classification implies softmax instead of sigmoid \n",
    "    Inputs:\n",
    "    X : shape(num_examples, 28, 28)\n",
    "    w : shape(784, 10)\n",
    "    b : shape(10, 1)\n",
    "    '''\n",
    "    x = tf.reshape(x, (-1, w.shape[0]))\n",
    "    output = tf.matmul(x, w) + b\n",
    "    softmax_output = softmax(output)\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy_loss(yhat, y):\n",
    "    ''' This function calculates the prediction prob of the correct class & takes log '''\n",
    "    # depth corresponds to number of classes\n",
    "    y_one_hot = tf.one_hot(y, depth = yhat.shape[-1])  \n",
    "    \n",
    "    # Masking only correct prediction prob\n",
    "    prob = tf.boolean_mask(yhat, y_one_hot)\n",
    "\n",
    "    # ce : log(y*yhat) but y is always 1\n",
    "    return -tf.math.log(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, grads, lr, batch_size):\n",
    "    for param, grad in zip(params, grads):\n",
    "        param.assign_sub(lr*grad/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\PanduranganR\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_grad.py:563: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.identity instead.\nepoch : 0, training_loss : 41365.953125\nepoch : 1, training_loss : 36169.8828125\nepoch : 2, training_loss : 33681.37109375\nepoch : 3, training_loss : 32128.697265625\nepoch : 4, training_loss : 31172.6796875\nepoch : 5, training_loss : 30483.875\nepoch : 6, training_loss : 29824.80859375\nepoch : 7, training_loss : 29350.865234375\nepoch : 8, training_loss : 28793.83203125\nepoch : 9, training_loss : 28415.0546875\nepoch : 10, training_loss : 28299.59765625\nepoch : 11, training_loss : 27803.828125\nepoch : 12, training_loss : 27720.1484375\nepoch : 13, training_loss : 27376.7734375\nepoch : 14, training_loss : 27151.88671875\nepoch : 15, training_loss : 27040.375\nepoch : 16, training_loss : 26899.296875\nepoch : 17, training_loss : 26642.458984375\nepoch : 18, training_loss : 26580.419921875\nepoch : 19, training_loss : 26423.1015625\nepoch : 20, training_loss : 26243.16796875\nepoch : 21, training_loss : 26280.525390625\nepoch : 22, training_loss : 26051.21484375\nepoch : 23, training_loss : 25980.48828125\nepoch : 24, training_loss : 25835.33203125\nepoch : 25, training_loss : 25798.90625\nepoch : 26, training_loss : 25694.669921875\nepoch : 27, training_loss : 25589.69921875\nepoch : 28, training_loss : 25516.17578125\nepoch : 29, training_loss : 25442.70703125\nepoch : 30, training_loss : 25391.8671875\nepoch : 31, training_loss : 25350.9453125\nepoch : 32, training_loss : 25263.32421875\nepoch : 33, training_loss : 25229.359375\nepoch : 34, training_loss : 25275.283203125\nepoch : 35, training_loss : 25166.916015625\nepoch : 36, training_loss : 25014.083984375\nepoch : 37, training_loss : 24978.328125\nepoch : 38, training_loss : 24979.861328125\nepoch : 39, training_loss : 24927.95703125\nepoch : 40, training_loss : 24839.123046875\nepoch : 41, training_loss : 24892.0625\nepoch : 42, training_loss : 24842.103515625\nepoch : 43, training_loss : 24735.568359375\nepoch : 44, training_loss : 24695.37109375\nepoch : 45, training_loss : 24702.576171875\nepoch : 46, training_loss : 24568.38671875\nepoch : 47, training_loss : 24665.826171875\nepoch : 48, training_loss : 24556.88671875\nepoch : 49, training_loss : 24457.1171875\n"
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "lr = 0.05\n",
    "batch_size = 256\n",
    "\n",
    "w, b = init_params(num_inputs, num_outputs)\n",
    "data_iter = load_data((train_features, train_labels), batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = log_reg_multiclass(X, w, b)\n",
    "            loss = crossentropy_loss(yhat, y)\n",
    "        # print(loss)\n",
    "        \n",
    "        # Calculate Gradients\n",
    "        dw, db = tape.gradient(loss, [w, b])\n",
    "        \n",
    "        # Update Gradients\n",
    "        sgd([w, b], [dw, db], lr, batch_size)\n",
    "    \n",
    "    training_loss = crossentropy_loss(log_reg_multiclass(train_features, w, b), train_labels)\n",
    "    print(f\"epoch : {epoch}, training_loss : {tf.reduce_sum(training_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8424"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "test_label_pred = log_reg_multiclass(test_features, w, b)\n",
    "test_pred = np.argmax(test_label_pred, axis = 1)\n",
    "accuracy_score(test_pred, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "train_features = tf.reshape(train_features, (-1, num_inputs))\n",
    "train_features = tf.cast(train_features, tf.float32)/255.0\n",
    "test_features = tf.reshape(test_features, (-1, num_inputs))\n",
    "test_features = tf.cast(test_features, tf.float32)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.initializers.RandomNormal(stddev = 0.1)\n",
    "log_reg_tf = tf.keras.Sequential()\n",
    "log_reg_tf.add(tf.keras.layers.Flatten())\n",
    "log_reg_tf.add(tf.keras.layers.Dense(10, kernel_initializer=initializer, activation  = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch : 0, training_loss : 0.7303739786148071\nepoch : 1, training_loss : 0.6314913034439087\nepoch : 2, training_loss : 0.5772733688354492\nepoch : 3, training_loss : 0.5531703233718872\nepoch : 4, training_loss : 0.532720148563385\nepoch : 5, training_loss : 0.5153622031211853\nepoch : 6, training_loss : 0.5039476156234741\nepoch : 7, training_loss : 0.5009585022926331\nepoch : 8, training_loss : 0.49021270871162415\nepoch : 9, training_loss : 0.48346146941185\nepoch : 10, training_loss : 0.476067453622818\nepoch : 11, training_loss : 0.46961405873298645\nepoch : 12, training_loss : 0.4683119058609009\nepoch : 13, training_loss : 0.4660100042819977\nepoch : 14, training_loss : 0.4672020375728607\nepoch : 15, training_loss : 0.4557550251483917\nepoch : 16, training_loss : 0.4568466246128082\nepoch : 17, training_loss : 0.4495241641998291\nepoch : 18, training_loss : 0.4520266354084015\nepoch : 19, training_loss : 0.4443742334842682\nepoch : 20, training_loss : 0.44346895813941956\nepoch : 21, training_loss : 0.44139614701271057\nepoch : 22, training_loss : 0.44093841314315796\nepoch : 23, training_loss : 0.4476941227912903\nepoch : 24, training_loss : 0.43475452065467834\nepoch : 25, training_loss : 0.43518269062042236\nepoch : 26, training_loss : 0.43460509181022644\nepoch : 27, training_loss : 0.4338182806968689\nepoch : 28, training_loss : 0.43019407987594604\nepoch : 29, training_loss : 0.43716123700141907\nepoch : 30, training_loss : 0.4272833466529846\nepoch : 31, training_loss : 0.42716386914253235\nepoch : 32, training_loss : 0.4279657006263733\nepoch : 33, training_loss : 0.42460912466049194\nepoch : 34, training_loss : 0.42422255873680115\nepoch : 35, training_loss : 0.4247973561286926\nepoch : 36, training_loss : 0.42437607049942017\nepoch : 37, training_loss : 0.4221900403499603\nepoch : 38, training_loss : 0.4183175563812256\nepoch : 39, training_loss : 0.4190676510334015\nepoch : 40, training_loss : 0.42024919390678406\nepoch : 41, training_loss : 0.4162634015083313\nepoch : 42, training_loss : 0.4159891903400421\nepoch : 43, training_loss : 0.4163571000099182\nepoch : 44, training_loss : 0.41471022367477417\nepoch : 45, training_loss : 0.4130130112171173\nepoch : 46, training_loss : 0.41578206419944763\nepoch : 47, training_loss : 0.4142831265926361\nepoch : 48, training_loss : 0.41256123781204224\nepoch : 49, training_loss : 0.4103928208351135\n"
    }
   ],
   "source": [
    "data_iter = load_data((train_features, train_labels), batch_size)\n",
    "\n",
    "num_epochs = 50\n",
    "lr = 0.05\n",
    "batch_size = 256\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = log_reg_tf(X, training = True)\n",
    "            ce_loss = loss(y, yhat)\n",
    "        \n",
    "        grads = tape.gradient(ce_loss, log_reg_tf.trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, log_reg_tf.trainable_variables))\n",
    "\n",
    "    yhat = log_reg_tf(train_features)\n",
    "    training_loss = loss(train_labels, yhat)\n",
    "    print(f\"epoch : {epoch}, training_loss : {tf.reduce_sum(training_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8421"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "test_label_pred = log_reg_tf(test_features)\n",
    "test_pred = np.argmax(test_label_pred, axis = 1)\n",
    "accuracy_score(test_pred, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)",
   "language": "python",
   "name": "python_defaultSpec_1600062567568"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}