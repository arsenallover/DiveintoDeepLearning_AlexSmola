{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600134758923",
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import hashlib\n",
    "\n",
    "DATA_HUB = dict()  #@save\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'  #@save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(name, cache_dir=os.path.join('..', 'data')):  #@save\n",
    "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    d2l.mkdir_if_not_exist(cache_dir)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # Hit cache\n",
    "    print(f'Downloading {fname} from {url}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_extract(name, folder=None):  #@save\n",
    "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    if ext == '.zip':\n",
    "        fp = zipfile.ZipFile(fname, 'r')\n",
    "    elif ext in ('.tar', '.gz'):\n",
    "        fp = tarfile.open(fname, 'r')\n",
    "    else:\n",
    "        assert False, 'Only zip/tar files can be extracted.'\n",
    "    fp.extractall(base_dir)\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir\n",
    "\n",
    "def download_all():  #@save\n",
    "    \"\"\"Download all files in the DATA_HUB.\"\"\"\n",
    "    for name in DATA_HUB:\n",
    "        download(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If pandas is not installed, please uncomment the following line:\n",
    "# !pip install pandas\n",
    "\n",
    "%matplotlib inline\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUB['kaggle_house_train'] = (  #@save\n",
    "    DATA_URL + 'kaggle_house_pred_train.csv',\n",
    "    '585e9cc93e70b39160e7921475f9bcd7d31219ce')\n",
    "\n",
    "DATA_HUB['kaggle_house_test'] = (  #@save\n",
    "    DATA_URL + 'kaggle_house_pred_test.csv',\n",
    "    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading ..\\data\\kaggle_house_pred_train.csv from http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_train.csv...\nDownloading ..\\data\\kaggle_house_pred_test.csv from http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_test.csv...\n"
    }
   ],
   "source": [
    "train_data = pd.read_csv(download('kaggle_house_train'))\n",
    "test_data = pd.read_csv(download('kaggle_house_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1460, 81)\n(1459, 80)\n"
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n0   1          60       RL         65.0       WD        Normal     208500\n1   2          20       RL         80.0       WD        Normal     181500\n2   3          60       RL         68.0       WD        Normal     223500\n3   4          70       RL         60.0       WD       Abnorml     140000\n"
    }
   ],
   "source": [
    "print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "all_features[numeric_features] = all_features[numeric_features].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.get_dummies(all_features, dummy_na=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = train_data.shape[0]\n",
    "train_features = tf.constant(all_features[:n_train].values, dtype=tf.float32)\n",
    "test_features = tf.constant(all_features[n_train:].values, dtype=tf.float32)\n",
    "train_labels = tf.constant(\n",
    "    train_data.SalePrice.values.reshape(-1, 1), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def get_net():\n",
    "    net = tf.keras.models.Sequential()\n",
    "    net.add(tf.keras.layers.Dense(1, kernel_regularizer = tf.keras.regularizers.l2(0)))\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_rmse(y_true, y_pred):\n",
    "    # To further stabilize the value when the logarithm is taken, set the\n",
    "    # value less than 1 as 1\n",
    "    clipped_preds = tf.clip_by_value(y_pred, 1, float('inf'))\n",
    "    return tf.sqrt(tf.reduce_mean(loss(\n",
    "        tf.math.log(y_true), tf.math.log(clipped_preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, batch_size, is_train = True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(buffer_size = 1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7.481093\n6.783854\n6.377819\n6.089277\n5.8658514\n5.6828547\n5.528625\n5.3951416\n5.277648\n5.1722207\n5.0769157\n4.9905744\n4.910351\n4.8363504\n4.7675905\n4.703025\n4.642667\n4.5855556\n4.531809\n4.480516\n4.432027\n4.3855505\n4.3412795\n4.298783\n4.258199\n4.2192273\n4.181665\n4.1453395\n4.1105256\n4.0766225\n4.043927\n4.012403\n3.9817514\n3.952043\n3.923318\n3.8951156\n3.8679621\n3.841394\n3.8154852\n3.7903109\n3.7657144\n3.7417583\n3.718305\n3.6955266\n3.6730855\n3.6513274\n3.6298718\n3.6089153\n3.5884154\n3.568285\n"
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 50, 0.05, 32\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "net = get_net()\n",
    "data_iter = load_data((train_features, train_labels), batch_size)\n",
    "net.compile(loss = loss, optimizer = optimizer)\n",
    "train_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = net(X)\n",
    "            l = loss(y, yhat)\n",
    "        \n",
    "        params = net.trainable_variables\n",
    "        grads = tape.gradient(l, params)\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, params))\n",
    "    train_loss.append(log_rmse(train_labels, net(train_features)))\n",
    "    print(train_loss[epoch].numpy())       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = tf.concat([X_train, X_part], 0)\n",
    "            y_train = tf.concat([y_train, y_part], 0)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\nfold 1, train log rmse 4.759342, valid log rmse 4.773881\n1\nfold 2, train log rmse 4.751072, valid log rmse 4.761989\n2\nfold 3, train log rmse 4.742665, valid log rmse 4.757630\n3\nfold 4, train log rmse 4.748435, valid log rmse 4.724545\n4\nfold 5, train log rmse 4.739539, valid log rmse 4.742434\n"
    }
   ],
   "source": [
    "k, num_epochs, lr, weight_decay, batch_size = 5, 10, 5, 0, 64\n",
    "train_loss_sum, test_loss_sum = 0, 0\n",
    "for i in range(k):\n",
    "    print(i)\n",
    "    train_features_kf, train_labels_kf, test_features_kf, test_labels_kf = get_k_fold_data(k, i, train_features, train_labels)\n",
    "    net = get_net()\n",
    "\n",
    "    data_iter = load_data((train_features_kf, train_labels_kf), batch_size)\n",
    "    net.compile(loss = loss, optimizer = optimizer)\n",
    "    train_loss = []; test_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            with tf.GradientTape() as tape:\n",
    "                yhat = net(X)\n",
    "                l = loss(y, yhat)\n",
    "            \n",
    "            params = net.trainable_variables\n",
    "            grads = tape.gradient(l, params)\n",
    "\n",
    "            optimizer.apply_gradients(zip(grads, params))\n",
    "        train_loss.append(log_rmse(train_labels_kf, net(train_features_kf)))\n",
    "        test_loss.append(log_rmse(test_labels_kf, net(test_features_kf)))\n",
    "        # print(train_loss[epoch].numpy())       \n",
    "    \n",
    "    train_loss_sum += train_loss[-1]\n",
    "    test_loss_sum += test_loss[-1]\n",
    "    print(f'fold {i + 1}, train log rmse {float(train_loss[-1]):f}, '\n",
    "            f'valid log rmse {float(test_loss[-1]):f}')\n",
    "\n",
    "avg_train_loss = train_loss_sum/k\n",
    "avg_test_loss = test_loss_sum/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4.752095 4.7482104\n"
    }
   ],
   "source": [
    "print(avg_test_loss.numpy(), avg_train_loss.numpy())"
   ]
  }
 ]
}