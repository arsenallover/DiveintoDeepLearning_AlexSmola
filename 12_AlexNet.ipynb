{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "display_name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "metadata": {
    "interpreter": {
     "hash": "22b3a9e511cc71c7e8c0c199ed3ba2f3f697f01009167188cfc9ddfa1a3c5b27"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net():\n",
    "    return tf.keras.models.Sequential([\n",
    "        # Here, we use a larger 11 x 11 window to capture objects. At the same\n",
    "        # time, we use a stride of 4 to greatly reduce the height and width of\n",
    "        # the output. Here, the number of output channels is much larger than\n",
    "        # that in LeNet\n",
    "        tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4,\n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "        # Make the convolution window smaller, set padding to 2 for consistent\n",
    "        # height and width across the input and output, and increase the\n",
    "        # number of output channels\n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding='same',\n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "        # Use three successive convolutional layers and a smaller convolution\n",
    "        # window. Except for the final convolutional layer, the number of\n",
    "        # output channels is further increased. Pooling layers are not used to\n",
    "        # reduce the height and width of input after the first two\n",
    "        # convolutional layers\n",
    "        tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',\n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',\n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same',\n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        # Here, the number of outputs of the fully-connected layer is several\n",
    "        # times larger than that in LeNet. Use the dropout layer to mitigate\n",
    "        # overfitting\n",
    "        tf.keras.layers.Dense(4096, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(4096, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Output layer. Since we are using Fashion-MNIST, the number of\n",
    "        # classes is 10, instead of 1000 as in the paper\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Conv2D output_shape: \t (1, 54, 54, 96)\nMaxPooling2D output_shape: \t (1, 26, 26, 96)\nConv2D output_shape: \t (1, 26, 26, 256)\nMaxPooling2D output_shape: \t (1, 12, 12, 256)\nConv2D output_shape: \t (1, 12, 12, 384)\nConv2D output_shape: \t (1, 12, 12, 384)\nConv2D output_shape: \t (1, 12, 12, 256)\nMaxPooling2D output_shape: \t (1, 5, 5, 256)\nFlatten output_shape: \t (1, 6400)\nDense output_shape: \t (1, 4096)\nDropout output_shape: \t (1, 4096)\nDense output_shape: \t (1, 4096)\nDropout output_shape: \t (1, 4096)\nDense output_shape: \t (1, 10)\n"
    }
   ],
   "source": [
    "X = tf.random.uniform((1, 224, 224, 1))\n",
    "for layer in net().layers:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output_shape: \\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, net, train_iter, test_iter, num_epochs):\n",
    "        self.net = net\n",
    "        self.train_iter = train_iter\n",
    "        self.test_iter = test_iter\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        test_acc = self.net.evaluate(self.test_iter, verbose = 0)\n",
    "        metrics = (logs[\"loss\"], logs[\"accuracy\"], test_acc[1])\n",
    "        print(f'epoch {epoch}, loss {metrics[0]:.3f}, train acc {metrics[1]:.3f}, 'f'test acc {metrics[2]:.3f}')\n",
    "\n",
    "        if epoch == self.num_epochs - 1:\n",
    "            print(f'loss {metrics[0]:.3f}, train acc {metrics[1]:.3f}, 'f'test acc {metrics[2]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None):   #@save\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    # Divide all numbers by 255 so that all pixel values are between\n",
    "    # 0 and 1, add a batch dimension at the last. And cast label to int32\n",
    "    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,\n",
    "                            tf.cast(y, dtype='int32'))\n",
    "    resize_fn = lambda X, y: (\n",
    "        tf.image.resize_with_pad(X, resize, resize) if resize else X, y)\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(\n",
    "            batch_size).shuffle(len(mnist_train[0])).map(resize_fn),\n",
    "        tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(\n",
    "            batch_size).map(resize_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize = 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.9, 10\n",
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "with strategy.scope():\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    net_model = net()\n",
    "    net_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "callback = TrainCallback(net_model, train_iter, test_iter, num_epochs)\n",
    "net_model.fit(train_iter, epochs=num_epochs, verbose=0, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}