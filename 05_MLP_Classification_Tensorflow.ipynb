{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600061577841",
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(train_features, train_target), (test_features, test_target) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "batch_size = 256\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "train_features = tf.reshape(train_features, (-1, num_inputs))\n",
    "train_features = tf.cast(train_features, tf.float32)/255.0\n",
    "test_features = tf.reshape(test_features, (-1, num_inputs))\n",
    "test_features = tf.cast(test_features, tf.float32)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(buffer_size = 100)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.initializers.RandomNormal(stddev = 0.1)\n",
    "net = tf.keras.Sequential()\n",
    "net.add(tf.keras.layers.Flatten())\n",
    "net.add(tf.keras.layers.Dense(50, activation = 'relu', kernel_initializer = initializer))\n",
    "net.add(tf.keras.layers.Dense(10, activation  = 'softmax', kernel_initializer=initializer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate= 0.05)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch : 0, training_loss : 2.071408271789551\nepoch : 1, training_loss : 1.9379944801330566\nepoch : 2, training_loss : 1.9183173179626465\nepoch : 3, training_loss : 1.9102270603179932\nepoch : 4, training_loss : 1.9035370349884033\nepoch : 5, training_loss : 1.847018837928772\nepoch : 6, training_loss : 1.8147729635238647\nepoch : 7, training_loss : 1.7952442169189453\nepoch : 8, training_loss : 1.781911015510559\nepoch : 9, training_loss : 1.7725396156311035\nepoch : 10, training_loss : 1.7656008005142212\nepoch : 11, training_loss : 1.7603390216827393\nepoch : 12, training_loss : 1.7559789419174194\nepoch : 13, training_loss : 1.752500295639038\nepoch : 14, training_loss : 1.7496061325073242\nepoch : 15, training_loss : 1.747083306312561\nepoch : 16, training_loss : 1.7449235916137695\nepoch : 17, training_loss : 1.74298095703125\nepoch : 18, training_loss : 1.741309642791748\nepoch : 19, training_loss : 1.7397739887237549\nepoch : 20, training_loss : 1.738404631614685\nepoch : 21, training_loss : 1.737160325050354\nepoch : 22, training_loss : 1.7359955310821533\nepoch : 23, training_loss : 1.7348942756652832\nepoch : 24, training_loss : 1.7339203357696533\nepoch : 25, training_loss : 1.7329803705215454\nepoch : 26, training_loss : 1.73207426071167\nepoch : 27, training_loss : 1.7312058210372925\nepoch : 28, training_loss : 1.7304441928863525\nepoch : 29, training_loss : 1.7296781539916992\nepoch : 30, training_loss : 1.7289398908615112\nepoch : 31, training_loss : 1.728334665298462\nepoch : 32, training_loss : 1.7275773286819458\nepoch : 33, training_loss : 1.7269424200057983\nepoch : 34, training_loss : 1.7264598608016968\nepoch : 35, training_loss : 1.7258073091506958\nepoch : 36, training_loss : 1.7252370119094849\nepoch : 37, training_loss : 1.7246965169906616\nepoch : 38, training_loss : 1.7242718935012817\nepoch : 39, training_loss : 1.723740577697754\nepoch : 40, training_loss : 1.7232513427734375\nepoch : 41, training_loss : 1.7228364944458008\nepoch : 42, training_loss : 1.7224502563476562\nepoch : 43, training_loss : 1.7219700813293457\nepoch : 44, training_loss : 1.7216192483901978\nepoch : 45, training_loss : 1.7211976051330566\nepoch : 46, training_loss : 1.7208391427993774\nepoch : 47, training_loss : 1.7204359769821167\nepoch : 48, training_loss : 1.7201030254364014\nepoch : 49, training_loss : 1.7198026180267334\n"
    }
   ],
   "source": [
    "num_epoch = 50\n",
    "batch_size = 256\n",
    "data_iter = load_data((train_features, train_target), batch_size)\n",
    "for epoch in range(num_epoch):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = net(X, training = True)\n",
    "            ce_loss = loss(y, yhat)\n",
    "            \n",
    "        # Gradient\n",
    "        params = net.trainable_variables\n",
    "        grads = tape.gradient(ce_loss, params)\n",
    "\n",
    "        # Update Gradient\n",
    "        optimizer.apply_gradients(zip(grads, params))\n",
    "    \n",
    "    training_loss = loss(train_target, net(train_features))\n",
    "    print(f\"epoch : {epoch}, training_loss : {training_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7393"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ytest_pred = np.argmax(net(test_features), axis = 1)\n",
    "accuracy_score(ytest_pred, test_target)"
   ]
  }
 ]
}