{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600497347489",
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n32768/29515 [=================================] - 0s 1us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26427392/26421880 [==============================] - 6s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n8192/5148 [===============================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4423680/4422102 [==============================] - 1s 0us/step\n"
    }
   ],
   "source": [
    "(train_features, train_target), (test_features, test_target) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "batch_size = 256\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "train_features = tf.reshape(train_features, (-1, num_inputs))\n",
    "train_features = tf.cast(train_features, tf.float32)/255.0\n",
    "test_features = tf.reshape(test_features, (-1, num_inputs))\n",
    "test_features = tf.cast(test_features, tf.float32)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(buffer_size = 100)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.initializers.RandomNormal(stddev = 0.1)\n",
    "net = tf.keras.Sequential()\n",
    "net.add(tf.keras.layers.Flatten())\n",
    "net.add(tf.keras.layers.Dense(50, activation = 'relu', kernel_initializer = initializer))\n",
    "net.add(tf.keras.layers.Dense(10, activation  = 'softmax', kernel_initializer=initializer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate= 0.05)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch : 0, training_loss : 1.957460880279541\nepoch : 1, training_loss : 1.8670094013214111\nepoch : 2, training_loss : 1.8441683053970337\nepoch : 3, training_loss : 1.8277978897094727\nepoch : 4, training_loss : 1.8086549043655396\nepoch : 5, training_loss : 1.791794776916504\nepoch : 6, training_loss : 1.7800219058990479\nepoch : 7, training_loss : 1.771610140800476\nepoch : 8, training_loss : 1.7653849124908447\nepoch : 9, training_loss : 1.7603974342346191\nepoch : 10, training_loss : 1.756365418434143\nepoch : 11, training_loss : 1.752997636795044\nepoch : 12, training_loss : 1.750098705291748\nepoch : 13, training_loss : 1.747572422027588\nepoch : 14, training_loss : 1.745384931564331\nepoch : 15, training_loss : 1.7434619665145874\nepoch : 16, training_loss : 1.7416846752166748\nepoch : 17, training_loss : 1.7401750087738037\nepoch : 18, training_loss : 1.7387263774871826\nepoch : 19, training_loss : 1.7374794483184814\nepoch : 20, training_loss : 1.7362146377563477\nepoch : 21, training_loss : 1.7350678443908691\nepoch : 22, training_loss : 1.734068512916565\nepoch : 23, training_loss : 1.7331347465515137\nepoch : 24, training_loss : 1.7322155237197876\nepoch : 25, training_loss : 1.7313237190246582\nepoch : 26, training_loss : 1.730452299118042\nepoch : 27, training_loss : 1.7297228574752808\nepoch : 28, training_loss : 1.7289003133773804\nepoch : 29, training_loss : 1.7282631397247314\nepoch : 30, training_loss : 1.727518081665039\nepoch : 31, training_loss : 1.7268881797790527\nepoch : 32, training_loss : 1.7262320518493652\nepoch : 33, training_loss : 1.7257285118103027\nepoch : 34, training_loss : 1.7250897884368896\nepoch : 35, training_loss : 1.7245512008666992\nepoch : 36, training_loss : 1.724045753479004\nepoch : 37, training_loss : 1.7234749794006348\nepoch : 38, training_loss : 1.7230522632598877\nepoch : 39, training_loss : 1.7225474119186401\nepoch : 40, training_loss : 1.7220808267593384\nepoch : 41, training_loss : 1.7216607332229614\nepoch : 42, training_loss : 1.7211825847625732\nepoch : 43, training_loss : 1.7207636833190918\nepoch : 44, training_loss : 1.7203893661499023\nepoch : 45, training_loss : 1.7199383974075317\nepoch : 46, training_loss : 1.7196658849716187\nepoch : 47, training_loss : 1.7192803621292114\nepoch : 48, training_loss : 1.7189500331878662\nepoch : 49, training_loss : 1.7185615301132202\n"
    }
   ],
   "source": [
    "num_epoch = 50\n",
    "batch_size = 256\n",
    "data_iter = load_data((train_features, train_target), batch_size)\n",
    "for epoch in range(num_epoch):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = net(X, training = True)\n",
    "            ce_loss = loss(y, yhat)\n",
    "            \n",
    "        # Gradient\n",
    "        params = net.trainable_variables\n",
    "        grads = tape.gradient(ce_loss, params)\n",
    "\n",
    "        # Update Gradient\n",
    "        optimizer.apply_gradients(zip(grads, params))\n",
    "    \n",
    "    training_loss = loss(train_target, net(train_features))\n",
    "    print(f\"epoch : {epoch}, training_loss : {training_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7426"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ytest_pred = np.argmax(net(test_features), axis = 1)\n",
    "accuracy_score(ytest_pred, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}